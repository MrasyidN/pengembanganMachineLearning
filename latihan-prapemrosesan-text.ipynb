{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# latihan pra-pemrosesan teks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. case folding\n",
    "- untuk mengubah semua huruf menjadi kebil atau besar\n",
    "2. removal special characters\n",
    "- menghapus angka\n",
    "- menghapus tanda baca\n",
    "- menghapus white space\n",
    "- menggunakan strip()\n",
    "- menggunakan replace()\n",
    "3. stopword removal(filtering)\n",
    "- stopword NLTK\n",
    "- stopword Sastrawi\n",
    "4. tokenizing\n",
    "- tokenisasi kata\n",
    "- tokenisasi kalimat\n",
    "- tokenisasi frasa\n",
    "- tokenisasi berdasarkan aturan(rule based tokenization)\n",
    "- tokenisasi berdasarkan model(model based tokenization)\n",
    "5. stemming (menghapus imbuhan)\n",
    "6. lemmatization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## case folding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yang sudah diubah:  ini adalah contoh teks\n"
     ]
    }
   ],
   "source": [
    "teks_asli = \"Ini Adalah Contoh Teks\"\n",
    "\n",
    "teks_lowecase = teks_asli.lower()\n",
    "\n",
    "print(\"yang sudah diubah:\", teks_lowecase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## removal special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### menghapus angka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setelah dihapus: ini adaldah contoh teks dengan angka  yang akan dihapus\n"
     ]
    }
   ],
   "source": [
    "def hapus_angka(teks):\n",
    "    teks_tanpa_angka = ''.join ([char for char in teks if not char.isdigit()])\n",
    "    return teks_tanpa_angka\n",
    "\n",
    "teks_dengan_angka = \"ini adaldah contoh teks dengan angka 12341234 yang akan dihapus\"\n",
    "\n",
    "teks_tanpa_angka = hapus_angka(teks_dengan_angka)\n",
    "\n",
    "print(\"setelah dihapus:\", teks_tanpa_angka)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### menghapus tanda baca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dalam dunia ini banyak hal terjadi dari yang kecil hingga yang besar Kita bisa melihat keindahan tapi juga kekejaman Ada harapan namun juga keputusasaan Bagaimanapun hidup terus berjalan tak peduli apa pun yang terjadi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Membuat set yang berisi semua tanda baca\n",
    "    punctuation_set = set(string.punctuation)\n",
    " \n",
    "    # Menghapus tanda baca dari teks\n",
    "    text_without_punctuation = ''.join(char for char in text if char not in punctuation_set)\n",
    " \n",
    "    return text_without_punctuation\n",
    "\n",
    "teks_asli = \"\"\"\n",
    "Dalam dunia ini, banyak hal terjadi, dari yang kecil hingga yang besar. Kita bisa melihat keindahan, tapi juga kekejaman. Ada harapan, namun juga keputusasaan. Bagaimanapun, hidup terus berjalan, tak peduli apa pun yang terjadi!\n",
    "\"\"\"\n",
    "\n",
    "# menghapus tanda baca\n",
    "teks_tanpa_tanda_baca = remove_punctuation(teks_asli)\n",
    "\n",
    "print(teks_tanpa_tanda_baca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### menghapus whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ini adalah contoh kalimat dengan spasi di awal dan akhir.\n"
     ]
    }
   ],
   "source": [
    "teks = \"   Ini adalah contoh kalimat dengan spasi di awal dan akhir.    \"\n",
    "teks_setelah_strip = teks.strip()\n",
    "print(teks_setelah_strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniadalahcontohkalimatdenganspasididalamnya.\n"
     ]
    }
   ],
   "source": [
    "teks_dengan_whitespace = \"Ini adalah    contoh kalimat    dengan spasi    di dalamnya.\"\n",
    "teks_tanpa_whitespace = teks_dengan_whitespace.replace(\" \", \"\")\n",
    "print(teks_tanpa_whitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perekonomian Indonesia pertumbuhan membanggakan .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\"\n",
    "\n",
    "tokens_data = word_tokenize(teks)\n",
    "\n",
    "# stopwords bahasa indonesia\n",
    "stopwords_indonesia = set(stopwords.words('indonesian'))\n",
    "\n",
    "kata_penting = [ kata for kata in tokens_data if kata.lower() not in stopwords_indonesia]\n",
    "\n",
    "#menggabungkan kata penting \n",
    "teks_tanpa_stopwords = ' '.join(kata_penting)\n",
    "\n",
    "print(teks_tanpa_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini', 'adalah', 'contoh', 'kalimat', 'untuk', 'tokenisasi', 'kata']\n"
     ]
    }
   ],
   "source": [
    "text = \"Ini adalah contoh kalimat untuk tokenisasi kata\"\n",
    "phrases = text.split(' ')\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentences tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini adalah contoh kalimat pertama.', ' Dan ini adalah contoh kalimat kedua.', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Ini adalah contoh kalimat pertama. Dan ini adalah contoh kalimat kedua.\"\n",
    "\n",
    "sentences = re.split(r'(?<=[.!?])+', text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phrase tokenization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apel', ' jeruk', ' pisang', ' dan mangga.']\n"
     ]
    }
   ],
   "source": [
    "text_buah = \"Apel, jeruk, pisang, dan mangga.\"\n",
    "\n",
    "phrases_buah = text_buah.split(',')\n",
    "print(phrases_buah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based Tokenizatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pertama', 'kita', 'perlu', 'menyiapkan', 'bahan', 'bahan', 'yang', 'diperlukan']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan.\"\n",
    "tokens = re.findall(r'\\w+|\\d+', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini', 'adalah', 'contoh', 'tokenisasi', 'berbasis', 'model.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Ini adalah contoh tokenisasi berbasis model.\"\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'easili', 'bought', 'cri', 'leav']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"easily\", \"bought\", \"crying\", \"leaves\"]\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'easily', 'buy', 'cry', 'leave']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"easily\", \"bought\", \"crying\", \"leaves\"]\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
